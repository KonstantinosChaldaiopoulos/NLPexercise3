{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Bellow is the answer of question 9. In question 9 we were asked to use ChatGPT  to perform NER,POS tagging and Chunking using zero-shot prompting. ChatGPT 3.5 was used. Bellow are the results:\n",
        "\n",
        "To read for zero-shot prompting you can check:\n",
        "\n",
        "https://www.promptingguide.ai/techniques/zeroshot"
      ],
      "metadata": {
        "id": "SMKr9P9_De_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of zero-shot prompting, ChatGPT-3.5 has shown GOODperformance across tasks like Named Entity Recognition, Part-of-Speech tagging, and text chunking. Its outputs are generally accurate and more human-readable compared to a detailed, token-level approach of BERT. However, the presentation of ChatGPT-3.5's results sometimes lacks consistency and detail, which can make interpretation challenging. Improving this aspect of the output could enhance its effectiveness in zero-shot tasks."
      ],
      "metadata": {
        "id": "vh9gJiG1SZSq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is tagging in a sentence of our choice."
      ],
      "metadata": {
        "id": "egNJEmzSnHup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Named edity recognition tagging\n",
        "---------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "The prompt used:\n",
        "\n",
        "=============================================\n",
        "\n",
        "Provide the named entity regognition (NER) tags in the text.\n",
        "\n",
        "Text: Mount Everest, part of the Himalayas, is the Earth's highest mountain above sea level.\n",
        "\n",
        "NER tagging:\n",
        "\n",
        "============================================\n",
        "\n",
        "Chat GPT answer:\n",
        "\n",
        "============================================\n",
        "\n",
        "NER tagging for the given text:\n",
        "\n",
        "Mount Everest, part of the Himalayas, is the Earth's highest mountain above sea level.\n",
        "\n",
        "Location: Mount Everest, Himalayas, Earth\n",
        "\n",
        "============================================"
      ],
      "metadata": {
        "id": "4vsFR-p8VzJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Despite accurately tagging most entities, the BERT model misclassified \"Everest\" as the beginning of a new location entity and failed to recognize \"Earth\" as a location. \n",
        "The Named Entity Recognition (NER) tagging by GPT-3.5 for the sentence: \"Mount Everest, part of the Himalayas, is the Earth's highest mountain above sea level\" correctly identifies \"Mount Everest,\" \"Himalayas,\" and \"Earth\" as locations. However, the format of the GPT-3.5's output is less structured compared to the output of the structure we created in the exercise although more user-friendly."
      ],
      "metadata": {
        "id": "ZZo1PNPVs9kG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part of speech tagging\n",
        "----------------------------------------------\n",
        "\n",
        "The prompt used:\n",
        "\n",
        "=============================================\n",
        "\n",
        "Provide the part of speech (POS) tags in the text.\n",
        "\n",
        "Text: Mount Everest, part of the Himalayas, is the Earth's highest mountain above sea level.\n",
        "\n",
        "POS tagging:\n",
        "\n",
        "=============================================\n",
        "\n",
        "Chat GPT answer :\n",
        "\n",
        "=============================================\n",
        "\n",
        "Mount/NNP Everest/NNP, /, part/NN of/IN the/DT Himalayas/NNP, /, is/VBZ the/DT Earth's/NNP highest/JJS mountain/NN above/IN sea/NN level/NN ./\n",
        "\n",
        "============================================="
      ],
      "metadata": {
        "id": "LPvOUdB9khZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon comparing the Part-of-Speech (POS) tagging delivered by GPT-3.5 with the BERT model's predictions, GPT-3.5 demonstrated superior accuracy, correctly identifying \"Earth\" as a proper noun (NNP). On the contrary, the BERT model incorrectly categorized \"Earth\" as a common noun (NN). However, GPT-3.5's output presentation was found to have inconsistencies and lacked a structured format, which might impact readability and interpretation adversely. If GPT-3.5 could adopt a more systematic and organized presentation similar to that of our exercises, it would significantly enhance its output clarity, leading to an improved overall user experience."
      ],
      "metadata": {
        "id": "z0V3nbfusCBc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text chunking\n",
        "--------------------------------------------\n",
        "\n",
        "The prompt used:\n",
        "\n",
        "=============================================\n",
        "\n",
        "Provide the chunking of the text.\n",
        "\n",
        "Text: Mount Everest, part of the Himalayas, is the Earth's highest mountain above sea level.\n",
        "\n",
        "Text chunking: \n",
        "\n",
        "============================================\n",
        "\n",
        "Chat GPT answer:\n",
        "\n",
        "============================================\n",
        "\n",
        "[Mount Everest] [part of] [the Himalayas], [is] [the Earth's highest mountain] [above sea level].\n",
        "\n",
        "============================================"
      ],
      "metadata": {
        "id": "9xKfU_Wqa6l0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compared to GPT-3.5, the BERT model breaks down the sentence in a more detailed and clear way. BERT maps each word to its role in the sentence, such as noun or verb phrases. This makes it easier to understand how each part of the sentence works together. Although GPT-3.5 also divides the sentence into smaller parts, it does not label each word. So, while both methods give us chunks of the sentence, BERT provides more information about how these chunks fit together grammatically."
      ],
      "metadata": {
        "id": "sSsyi_qQxMH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is tagging in sentences used to train the BERT model of previous questions."
      ],
      "metadata": {
        "id": "2LRhEuEGnCQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Named edity recognition tagging \n",
        "---------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "The prompt used:\n",
        "\n",
        "=============================================\n",
        "\n",
        "Provide the named entity regognition (NER) tags in the text.\n",
        "\n",
        "Text: Luxembourg's traditional Christmas market, which starts on Saturday and runs to December 24, has taken to the world wide web as a way of publicising its activities..\n",
        "\n",
        "NER tagging:\n",
        "\n",
        "============================================\n",
        "\n",
        "Chat GPT answer:\n",
        "\n",
        "============================================\n",
        "\n",
        "Luxembourg's traditional Christmas market (LOCATION), which starts on Saturday (DATE) and runs to December 24 (DATE), has taken to the world wide web as a way of publicising its activities.\n",
        "\n",
        "============================================\n",
        "\n",
        "Looking at the NER tags for the sentence about Luxembourg's Christmas market, we can see that GPT-3.5 correctly identified the locations and dates. In contrast, the BERT model made an error by misinterpreting 'world wide web' as separate entities. While BERT provides more detailed, token-level tagging, GPT-3.5 offers more human-readable tags by grouping related words together. This highlights the strengths and weaknesses of each model."
      ],
      "metadata": {
        "id": "FBfUTzt3F9qt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part of speech tagging\n",
        "----------------------------------------------\n",
        "\n",
        "The prompt used:\n",
        "\n",
        "=============================================\n",
        "\n",
        "Provide the part of speech (POS) tags in the text.\n",
        "\n",
        "Text: \"A meeting with the Pope is also planned,\" Dariusz Rosati told a news conference.\n",
        "\n",
        "POS tagging:\n",
        "\n",
        "=============================================\n",
        "\n",
        "Chat GPT answer :\n",
        "\n",
        "=============================================\n",
        "\n",
        "Text: \"A meeting with the Pope is also planned,\" Dariusz Rosati told a news conference.\n",
        "\n",
        "POS tagging:\n",
        "\n",
        "Article: A\n",
        "\n",
        "Noun: meeting\n",
        "\n",
        "Preposition: with\n",
        "\n",
        "Determiner: the\n",
        "\n",
        "Noun: Pope\n",
        "\n",
        "Verb: is\n",
        "\n",
        "Adverb: also\n",
        "\n",
        "Verb: planned\n",
        "\n",
        "Punctuation: ,\n",
        "\n",
        "Proper noun: Dariusz Rosati\n",
        "\n",
        "Verb: told\n",
        "\n",
        "Determiner: a\n",
        "\n",
        "Noun: news\n",
        "\n",
        "Noun: conference\n",
        "\n",
        "Punctuation: .\n",
        "\n",
        "============================================="
      ],
      "metadata": {
        "id": "OcuacE0aG99l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compared to our results with BERT, both GPT-3.5 and BERT are pretty good at figuring out the part of speech of words. They both get most of it right, but sometimes, they make small mistakes. GPT-3.5 is a bit better at telling the difference between common nouns and proper nouns (like names). BERT sometimes gets this wrong. But all in all, both of them do a great job."
      ],
      "metadata": {
        "id": "bDciFR1bQ87I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text chunking\n",
        "--------------------------------------------\n",
        "\n",
        "The prompt used:\n",
        "\n",
        "=============================================\n",
        "\n",
        "Provide the chunking of the text.\n",
        "\n",
        "Text: On Friday, for their friendly against Scotland at Murrayfield, more than a year after the 30-year-old wing announced he was retiring following differences over selection.\n",
        "\n",
        "Text chunking: \n",
        "\n",
        "============================================\n",
        "\n",
        "Chat GPT answer:\n",
        "\n",
        "============================================\n",
        "\n",
        "\n",
        "Chunking is a natural language processing task that involves dividing a sentence or a text into smaller syntactic components called chunks. These chunks are often phrases or groups of words that function together in a sentence. Here's the chunking of the given text:\n",
        "\n",
        "[On Friday], [for their friendly against Scotland at Murrayfield], [more than a year after], [the 30-year-old wing], [announced], [he was retiring], [following differences over selection].\n",
        "\n",
        "Note : The chunks are enclosed within square brackets and are separated based on their syntactic boundaries.\n",
        "\n",
        "============================================"
      ],
      "metadata": {
        "id": "Cj8NtQc6Hrh2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT and GPT-3.5 both chunk text effectively, but BERT provides an extra layer of detail by labeling each word's role, making it easier to understand sentence structure. In contrast, GPT-3.5 chunks sentences without labeling each word, making it less comprehensive than BERT."
      ],
      "metadata": {
        "id": "UPQEU4anR1sj"
      }
    }
  ]
}